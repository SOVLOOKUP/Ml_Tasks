{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树：1.特征选择\n",
    "      2.决策树生成\n",
    "      3.决策树的修剪\n",
    "\n",
    "(ID3,C4.5)决策树算法伪代码:\n",
    "\n",
    "输入:数据集D，特征集合A，阈值e\n",
    "输出:决策树T\n",
    "如果D中所有实例输出同一类 𝐶𝑘 , 则T作为单节点树，并将类 𝐶𝑘 作为该节点的类标记，返回T;\n",
    "若 𝐴=∅ ,则T为单节点树，将D中实例数最多的类 𝐶𝑘 作为该节点的类标记，返回T；\n",
    "否则，根据信息增益(ID3)或者信息增益比(C4.5)计算特征A对D的值，选择当前最优的特征 𝐴𝑔 ;\n",
    "如果 𝐴𝑔 的信息增益小于阈值e，则置T为单节点数，并将D中实例最多的类 𝐶𝑘 作为当前的类标记，返回T；\n",
    "否则，根据 𝐴𝑔 中的每一个不同的 𝑎𝑖 ,根据 𝐴𝑔=𝑎𝑖 将D分成若干个非空子集，对于第i个子节点，以 𝐷𝑖 为数据集，以 𝐴−𝐴𝑔 为特征集，递归(重复3-6)构造决策树 𝑇𝑖 ,返回 𝑇𝑖 .\n",
    "对决策树模型T进行剪枝."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numbers\n",
    "import warnings\n",
    "from math import ceil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import issparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree(object):\n",
    "    \"\"\"自定的树结构,用来保存决策树.\n",
    "    \n",
    "    Paramters:\n",
    "    ----------\n",
    "    col: int, default(-1)\n",
    "        当前使用的第几列数据\n",
    "    \n",
    "    val: int or float or str, 分割节点\n",
    "        分割节点的值, \n",
    "        int or float : 使用大于进行比较 \n",
    "        str : 使用等于模式\n",
    "    \n",
    "    LeftChild: DecisionTree\n",
    "        左子树, <= val\n",
    "    \n",
    "    RightChild: DecisionTree\n",
    "        右子树, > val\n",
    "    \n",
    "    results: \n",
    "    \"\"\"\n",
    "    def __init__(self, col=-1, val=None, LeftChild=None, RightChild=None, result=None):\n",
    "        self.col = col\n",
    "        self.val = val\n",
    "        self.LeftChild = LeftChild\n",
    "        self.RightChild = RightChild\n",
    "        self.result = result\n",
    "\n",
    "\n",
    "class DecisionTreeClassifier(object):\n",
    "    \"\"\"使用基尼指数的分类决策树接口.\n",
    "    \n",
    "    Paramters:\n",
    "    ---------\n",
    "    max_depth : int or None, optional(dafault=None)\n",
    "        表示决策树的最大深度. None: 表示不设置深度,可以任意扩展,\n",
    "        直到叶子节点的个数小于min_samples_split个数.\n",
    "\n",
    "    min_samples_split : int, optional(default=2)\n",
    "        表示最小分割样例数.\n",
    "        if int, 表示最小分割样例树,如果小于这个数字,不在进行分割.\n",
    "\n",
    "    min_samples_leaf : int, optional (default=1)\n",
    "        表示叶节点最少有min_samples_leaf个节点树,如果小于等于这个数,直接返回.\n",
    "        if int, min_samples_leaf就是最小样例数.\n",
    "\n",
    "    min_impurity_decrease : float, optional (default=0.)\n",
    "        分割之后基尼指数大于这个数,则进行分割.\n",
    "        N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
    "                        - N_t_L / N_t * left_impurity)\n",
    "\n",
    "    min_impurity_split : float, default=1e-7\n",
    "        停止增长的阈值,小于这个值直接返回.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    classes_ : array of shape (n_classes,) or a list of such arrays\n",
    "        表示所有的类\n",
    "    \n",
    "    feature_importances_ : ndarray of shape (n_features,)\n",
    "        特征重要性, 被选择最优特征的次数,进行降序.\n",
    "    \n",
    "    tree_ : Tree object\n",
    "        The underlying Tree object.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 max_depth=None,\n",
    "                 min_samples_split=2,\n",
    "                 min_samples_leaf=1,\n",
    "                 min_impurity_decrease=0.,\n",
    "                 min_impurity_split=1e-7):        \n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_impurity_decrease = min_impurity_decrease\n",
    "        self.min_impurity_split = min_impurity_split\n",
    "        self.classes_ = None\n",
    "        self.max_features_ = None\n",
    "        self.decision_tree = None\n",
    "        self.all_feats = None\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, check_input=True):\n",
    "        \"\"\"使用X和y训练决策树的分类模型.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like} of shape (n_samples, n_features)\n",
    "            The training input samples. Internally, it will be converted to\n",
    "            ``dtype=np.float32``\n",
    "            \n",
    "        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "            The target values (class labels) as integers or strings.\n",
    "        \n",
    "        check_input : bool, (default=True)\n",
    "            Allow to bypass several input checking.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Fitted estimator.\n",
    "        \"\"\"\n",
    "        if isinstance(X, list):\n",
    "            X = self.__check_array(X)\n",
    "        if isinstance(y, list):\n",
    "            y = self.__check_array(y)\n",
    "        if X.shape[0] != y.shape[0]:\n",
    "            raise ValueError(\"输入的数据X和y长度不匹配\")\n",
    "        \n",
    "        self.classes_ = list(set(y))\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        if isinstance(y, pd.DataFrame):\n",
    "            y = y.values\n",
    "        \n",
    "        data_origin = np.c_[X, y]\n",
    "#         print (data_origin)\n",
    "        self.all_feats = [i for i in range(X.shape[1])]\n",
    "        self.max_features_ = X.shape[0]\n",
    "        \n",
    "        data = copy.deepcopy(data_origin)\n",
    "        self.decision_tree = self.__build_tree(data, 0)\n",
    "\n",
    "\n",
    "    def __predict_one(self, input_x):\n",
    "        \"\"\"预测一个样例的返回结果.\n",
    "        \n",
    "        Paramters:\n",
    "        ---------\n",
    "        input_x : list or np.ndarray\n",
    "            需要预测输入数据\n",
    "        \n",
    "        Returns:\n",
    "        -------\n",
    "        class : 对应的类\n",
    "        \"\"\"\n",
    "        \n",
    "        tree = self.decision_tree\n",
    "        #============================= show me your code =======================\n",
    "        def run(input_x, tree):\n",
    "            \"\"\"内部使用函数\n",
    "            \n",
    "            \"\"\"\n",
    "            # 叶子节点返回\n",
    "            if tree.result != None: \n",
    "                return tree.result\n",
    "            v = input_x[tree.col]\n",
    "            branch = None\n",
    "            if isinstance(v, int) or isinstance(v, float):\n",
    "                if v <= tree.val: \n",
    "                    tree = tree.LeftChild\n",
    "                else: \n",
    "                    tree = tree.RightChild\n",
    "            elif isinstance(v, str):\n",
    "                if v == tree.val: \n",
    "                    tree = tree.LeftChild\n",
    "                else: \n",
    "                    tree = tree.RightChild\n",
    "            return run(input_x, tree)\n",
    "        pre_y = run(input_x, tree)\n",
    "        #============================= show me your code =======================\n",
    "        return pre_y\n",
    "    \n",
    "    \n",
    "    def predict(self, test):\n",
    "        \"\"\"预测函数,\n",
    "        \n",
    "        Paramters:\n",
    "        ---------\n",
    "        test: {array-like} of shape (n_samples, n_features)\n",
    "        \n",
    "        Returns:\n",
    "        result : np.array(list) \n",
    "        \"\"\"\n",
    "        result = []\n",
    "        for i in range(len(test)):\n",
    "            result.append(self.__predict_one(test[i]))\n",
    "        return np.array(result)\n",
    "    \n",
    "    \n",
    "    def score(self, vali_X, vali_y):\n",
    "        \"\"\"验证模型的特征,这里使用准确率.\n",
    "        Parameters\n",
    "        ----------\n",
    "        vali_X : {array-like} of shape (n_samples, n_features)\n",
    "            The training input samples. Internally, it will be converted to\n",
    "            ``dtype=np.float32``\n",
    "\n",
    "        vali_y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "            The target values (class labels) as integers or strings.\n",
    "        \n",
    "        Returns:\n",
    "        -------\n",
    "        score : float, 预测的准确率\n",
    "        \"\"\"\n",
    "        vali_y = np.array(vali_y)\n",
    "        pre_y = self.predict(vali_X)\n",
    "        pre_score = 1.0 * sum(vali_y == pre_y) / len(vali_y)\n",
    "        return pre_score\n",
    "    \n",
    "    \n",
    "    def __build_tree(self, data, depth):\n",
    "        \"\"\"创建决策树的主要代码\n",
    "        \n",
    "        Paramters:\n",
    "        ---------\n",
    "        data : {array-like} of shape (n_samples, n_features) + {label}\n",
    "            The training input samples. Internally, it will be converted to\n",
    "            ``dtype=np.float32``\n",
    "        \n",
    "        depth: int, 树的深度\n",
    "        \n",
    "        Returns:\n",
    "        -------\n",
    "        DecisionTree\n",
    "            \n",
    "        \"\"\"        \n",
    "        labels = np.unique(data[:,-1])\n",
    "        # 只剩下唯一的类别时,停止,返回对应类别\n",
    "        if len(labels) == 1:\n",
    "            return DecisionTree(result=list(labels)[0])\n",
    "        \n",
    "        # 遍历完所有特征时,只剩下label标签,就返回出现字数最多的类标签\n",
    "        if not self.all_feats:\n",
    "            return DecisionTree(result=np.argmax(np.bincount(data[:,-1].astype(int))))\n",
    "\n",
    "        # 超过最大深度,则停止,使用出现最多的参数作为该叶子节点的类\n",
    "        if self.max_depth and depth > self.max_depth:\n",
    "            return DecisionTree(result=np.argmax(np.bincount(data[:,-1].astype(int))))\n",
    "\n",
    "        # 如果剩余的样本数大于等于给定的参数 min_samples_split,\n",
    "        # 则不在进行分割, 直接返回类别中最多的类,该节点作为叶子节点\n",
    "        if self.min_samples_split >= data.shape[0]:\n",
    "            return DecisionTree(result=np.argmax(np.bincount(data[:,-1].astype(int))))\n",
    "\n",
    "        # 叶子节点个数小于指定参数就进行返回,叶子节点中的出现最多的类\n",
    "        if self.min_samples_leaf >= data.shape[0]:\n",
    "            return DecisionTree(result=np.argmax(np.bincount(data[:,-1].astype(int))))\n",
    "        \n",
    "        # 根据基尼指数选择每个分割的最优特征\n",
    "        best_idx, best_val, min_gini = self.__getBestFeature(data)\n",
    "#         print (\"Current best Feature:\", best_idx, best_val, min_gini)\n",
    "        # 如果当前的gini指数小于指定阈值,直接返回\n",
    "        if min_gini < self.min_impurity_split:\n",
    "            return DecisionTree(result=np.argmax(np.bincount(data[:,-1].astype(int))))\n",
    "        \n",
    "        leftData, rightData = self.__splitData(data, best_idx, best_val)\n",
    "        \n",
    "        #============================= show me your code =======================\n",
    "        leftDecisionTree = self.__build_tree(leftData, depth + 1)\n",
    "        rightDecisionTree = self.__build_tree(rightData, depth + 1)\n",
    "        #============================= show me your code =======================\n",
    "        \n",
    "        return DecisionTree(col=best_idx, val=best_val, LeftChild=leftDecisionTree, RightChild=rightDecisionTree)\n",
    "\n",
    "    \n",
    "    def __getBestFeature(self, data):\n",
    "        \"\"\"得到最优特征对应的列\n",
    "        Paramters:\n",
    "        ---------\n",
    "        data: np.ndarray\n",
    "            从data中选择最优特征\n",
    "            \n",
    "        Returns:\n",
    "        -------\n",
    "        bestInx, val, 最优特征的列的索引和使用的值.\n",
    "        \"\"\"\n",
    "        best_idx = -1\n",
    "        best_val = None\n",
    "        min_gini = 1.0                \n",
    "        # 遍历现在可以使用的特征列\n",
    "        #============================= show me your code =======================\n",
    "        for feat_idx in self.all_feats:\n",
    "            # 遍历所用的特征:\n",
    "            # 判断数据类型,貌似对numpy.ndarry不好有用\n",
    "            # numpy.ndarry的类型自动向上扩展\n",
    "            x = data[:, feat_idx]\n",
    "            for val in data[:, feat_idx]:\n",
    "                leftData, rightData = self.__splitData(data, feat_idx, val)\n",
    "                left_gini = self.gini(leftData[:, -1])\n",
    "                right_gini = self.gini(rightData[:, -1])\n",
    "#                 print (len(leftData), len(rightData), len(data))\n",
    "                cur_gini = 1.0 * len(leftData) / len(data) * left_gini\n",
    "                cur_gini += 1.0 * len(rightData) / len(data) * right_gini\n",
    "                \n",
    "                if cur_gini < min_gini:\n",
    "                    best_idx = feat_idx\n",
    "                    best_val = val\n",
    "                    min_gini = cur_gini\n",
    "        #============================= show me your code =======================\n",
    "        # 删除使用过的特征\n",
    "        self.all_feats.remove(best_idx)\n",
    "        \n",
    "        return best_idx, best_val, min_gini\n",
    "        \n",
    "    \n",
    "    def gini(self, labels):\n",
    "        \"\"\"计算基尼指数.\n",
    "        \n",
    "        Paramters:\n",
    "        ----------\n",
    "        labels: list or np.ndarray, 数据对应的类目集合.\n",
    "        \n",
    "        Returns: \n",
    "        -------\n",
    "        gini : float ``` Gini(p) = \\sum_{k=1}^{K}p_k(1-p_k)=1-\\sum_{k=1}^{K}p_k^2 ```\n",
    "        \n",
    "        \"\"\"\n",
    "        #============================= show me your code =======================\n",
    "        labelSet = np.array(labels)\n",
    "        length = labelSet.shape[0]\n",
    "        gini = 1.\n",
    "        classes = np.unique(labelSet)\n",
    "        for c in classes:\n",
    "            gini -= (1.0 * np.sum(labelSet == c) / length) ** 2\n",
    "        #============================= show me your code =======================\n",
    "        return gini\n",
    "    \n",
    "    \n",
    "    def __splitData(self, data, featColumn, val):\n",
    "        '''根据特征划分数据集分成左右两部分.\n",
    "        Paramters:\n",
    "        ---------\n",
    "        data: np.ndarray, 分割的数据\n",
    "        \n",
    "        featColumn : int, 使用第几列的数据进行分割\n",
    "        \n",
    "        val : int or float or str, 分割的值\n",
    "            int or float : 使用比较方式\n",
    "            str : 使用相等方式\n",
    "        \n",
    "        Returns:\n",
    "        -------\n",
    "        leftData, RightData\n",
    "            int or left: leftData <= val < rightData\n",
    "            str : leftData = val and rightData != val\n",
    "        '''\n",
    "        if isinstance(val, str):\n",
    "            leftData = data[data[:, featColumn] == val]\n",
    "            rightData = data[data[:, featColumn] != val]\n",
    "        elif isinstance(val, int) or isinstance(val, float):\n",
    "            leftData = data[data[:, featColumn] <= val]\n",
    "            rightData = data[data[:, featColumn] > val]\n",
    "        return leftData, rightData\n",
    "    \n",
    "    \n",
    "    def __check_array(self, X):\n",
    "        \"\"\"检查数据类型\n",
    "        Parameters:\n",
    "        ----------\n",
    "        X : {array-like} of shape (n_samples, n_features)\n",
    "            The training input samples.\n",
    "        \n",
    "        Retures\n",
    "        -------\n",
    "        X: {array-like} of shape (n_samples, n_features)\n",
    "        \"\"\"\n",
    "        if isinstance(X, list):\n",
    "            X = np.array(X)\n",
    "        if not isinstance(X, np.ndarray) and not isinstance(X, pd.DataFrame):\n",
    "            raise ValueError(\"输出数据不合法,目前只支持np.ndarray or pd.DataFrame\")\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Score: 0.9\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 分类树\n",
    "    X, y = load_iris(return_X_y=True)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    clf = DecisionTreeClassifier()\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print (\"Classifier Score:\", clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "666\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
